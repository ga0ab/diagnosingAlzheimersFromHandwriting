{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aab6d87-3615-4cdd-a0bd-4a6e4f47fa45",
   "metadata": {},
   "source": [
    "## This approach involved training and evaluating our four classifiers on 25 distinct feature sets, each corresponding to a specific handwriting task in the DARWIN data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f5937-4152-4a35-b1d9-62380fa08de4",
   "metadata": {},
   "source": [
    "### Here we do it for the GN Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe6533e-b58c-42b0-8016-778c0fdd3775",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We extract the feature vectors from each of the 25 tasks.\n",
    "'''\n",
    " \n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    " \n",
    "# Fetch dataset \n",
    "darwin = fetch_ucirepo(id=732)\n",
    " \n",
    "# Data (as pandas dataframes)\n",
    "X = darwin.data.features\n",
    "y = darwin.data.targets\n",
    " \n",
    "X = X.drop(columns=['ID'])\n",
    " \n",
    "# Number of attributes per task\n",
    "num_attributes_per_task = 18\n",
    " \n",
    "# Number of tasks\n",
    "num_tasks = 25\n",
    " \n",
    "# Create a dictionary to hold the DataFrames for each task\n",
    "task_dfs = {}\n",
    " \n",
    "# Create a dictionary to hold the labels for each task\n",
    "task_labels = {}\n",
    " \n",
    "# Iterate through the number of tasks\n",
    "for i in range(num_tasks):\n",
    "    # Column indices for the current task\n",
    "    start_index = i * num_attributes_per_task\n",
    "    end_index = start_index + num_attributes_per_task\n",
    "    # Select columns for the current task\n",
    "    task_columns = X.columns[start_index:end_index]\n",
    "    # Create a DataFrame for the current task\n",
    "    task_df = X[task_columns].copy()\n",
    "    # Store the DataFrame in the dictionary with the key 'task_i'\n",
    "    task_dfs[f'task_{i + 1}'] = task_df\n",
    "    # Select labels for the current task\n",
    "    task_labels[f'task_{i + 1}'] = y.copy()  # Labels are identical for all tasks, adjust if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5ac35c-cd9e-4c25-9893-a322dee502e6",
   "metadata": {},
   "source": [
    "### Performing grid search to find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0840d13c-ee38-4833-85cf-923e8ce9cc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_1: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters for task_2: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_3: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters for task_4: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters for task_5: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_6: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters for task_7: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_8: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters for task_9: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters for task_10: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters for task_11: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_12: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters for task_13: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_14: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters for task_15: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_16: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters for task_17: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters for task_18: {'var_smoothing': 1e-09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters for task_19: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters for task_20: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_21: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters for task_22: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters for task_23: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters for task_24: {'var_smoothing': 1e-09}\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters for task_25: {'var_smoothing': 1e-09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "grid search GAUSSIAN\n",
    "'''\n",
    " \n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    " \n",
    "# Define the parameter grid for GaussianNB\n",
    "param_grid = {\n",
    "   'var_smoothing': [1e-09, 1e-08, 1e-07, 1e-06, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01, 1]\n",
    "}\n",
    " \n",
    "# Dictionary to store the best parameters for each task\n",
    "best_params_per_task = {}\n",
    " \n",
    "# Iterate through tasks\n",
    "for task, task_df in task_dfs.items():\n",
    "   X_task = task_df\n",
    "   y_task = task_labels[task]\n",
    "  \n",
    "   # Split the data\n",
    "   X_train, X_test, y_train, y_test = train_test_split(X_task, y_task, test_size=0.2, random_state=42, stratify=y_task)\n",
    "  \n",
    "   # Initialize Gaussian Naive Bayes\n",
    "   gnb = GaussianNB()\n",
    "  \n",
    "   # Grid Search\n",
    "   grid_search = GridSearchCV(estimator=gnb, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1, scoring='f1')\n",
    "   grid_search.fit(X_train, y_train)\n",
    "  \n",
    "   # Store the best parameters for this task\n",
    "   best_params_per_task[task] = grid_search.best_params_\n",
    " \n",
    "   print(f\"Best Parameters for {task}: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e495f34-22e4-4168-aeb8-a00a4a9d40da",
   "metadata": {},
   "source": [
    "## The results given by the grid-search show that the optimal parameters are identical for each of the 25 tasks:\n",
    "\n",
    "**best_parameters: {'var_smoothing': 1e-09}**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061c87ac-fed7-43fb-967d-29e723adecf6",
   "metadata": {},
   "source": [
    "## Performance Evaluation of GN Bayes classifier, using the 20 runs method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83786799-7ede-4010-aef1-fdadb2bbe19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Metrics for task_1:\n",
      "Mean Accuracy: 0.6286\n",
      "Mean Precision: 0.7088\n",
      "Mean Recall: 0.4972\n",
      "Mean F1 Score: 0.5675\n",
      "Mean Sensitivity: 0.4972\n",
      "Mean Specificity: 0.7676\n",
      "\n",
      "\n",
      "Performance Metrics for task_2:\n",
      "Mean Accuracy: 0.7229\n",
      "Mean Precision: 0.8852\n",
      "Mean Recall: 0.5389\n",
      "Mean F1 Score: 0.6614\n",
      "Mean Sensitivity: 0.5389\n",
      "Mean Specificity: 0.9176\n",
      "\n",
      "\n",
      "Performance Metrics for task_3:\n",
      "Mean Accuracy: 0.6914\n",
      "Mean Precision: 0.8119\n",
      "Mean Recall: 0.5250\n",
      "Mean F1 Score: 0.6277\n",
      "Mean Sensitivity: 0.5250\n",
      "Mean Specificity: 0.8676\n",
      "\n",
      "\n",
      "Performance Metrics for task_4:\n",
      "Mean Accuracy: 0.6986\n",
      "Mean Precision: 0.8900\n",
      "Mean Recall: 0.4750\n",
      "Mean F1 Score: 0.6121\n",
      "Mean Sensitivity: 0.4750\n",
      "Mean Specificity: 0.9353\n",
      "\n",
      "\n",
      "Performance Metrics for task_5:\n",
      "Mean Accuracy: 0.7086\n",
      "Mean Precision: 0.8798\n",
      "Mean Recall: 0.5056\n",
      "Mean F1 Score: 0.6368\n",
      "Mean Sensitivity: 0.5056\n",
      "Mean Specificity: 0.9235\n",
      "\n",
      "\n",
      "Performance Metrics for task_6:\n",
      "Mean Accuracy: 0.7043\n",
      "Mean Precision: 0.8832\n",
      "Mean Recall: 0.4944\n",
      "Mean F1 Score: 0.6253\n",
      "Mean Sensitivity: 0.4944\n",
      "Mean Specificity: 0.9265\n",
      "\n",
      "\n",
      "Performance Metrics for task_7:\n",
      "Mean Accuracy: 0.7314\n",
      "Mean Precision: 0.7442\n",
      "Mean Recall: 0.7417\n",
      "Mean F1 Score: 0.7396\n",
      "Mean Sensitivity: 0.7417\n",
      "Mean Specificity: 0.7206\n",
      "\n",
      "\n",
      "Performance Metrics for task_8:\n",
      "Mean Accuracy: 0.7200\n",
      "Mean Precision: 0.8852\n",
      "Mean Recall: 0.5333\n",
      "Mean F1 Score: 0.6572\n",
      "Mean Sensitivity: 0.5333\n",
      "Mean Specificity: 0.9176\n",
      "\n",
      "\n",
      "Performance Metrics for task_9:\n",
      "Mean Accuracy: 0.7457\n",
      "Mean Precision: 0.9031\n",
      "Mean Recall: 0.5667\n",
      "Mean F1 Score: 0.6914\n",
      "Mean Sensitivity: 0.5667\n",
      "Mean Specificity: 0.9353\n",
      "\n",
      "\n",
      "Performance Metrics for task_10:\n",
      "Mean Accuracy: 0.7014\n",
      "Mean Precision: 0.8752\n",
      "Mean Recall: 0.4944\n",
      "Mean F1 Score: 0.6240\n",
      "Mean Sensitivity: 0.4944\n",
      "Mean Specificity: 0.9206\n",
      "\n",
      "\n",
      "Performance Metrics for task_11:\n",
      "Mean Accuracy: 0.6271\n",
      "Mean Precision: 0.6370\n",
      "Mean Recall: 0.7528\n",
      "Mean F1 Score: 0.6705\n",
      "Mean Sensitivity: 0.7528\n",
      "Mean Specificity: 0.4941\n",
      "\n",
      "\n",
      "Performance Metrics for task_12:\n",
      "Mean Accuracy: 0.5900\n",
      "Mean Precision: 0.6633\n",
      "Mean Recall: 0.5500\n",
      "Mean F1 Score: 0.5632\n",
      "Mean Sensitivity: 0.5500\n",
      "Mean Specificity: 0.6324\n",
      "\n",
      "\n",
      "Performance Metrics for task_13:\n",
      "Mean Accuracy: 0.6700\n",
      "Mean Precision: 0.7784\n",
      "Mean Recall: 0.5167\n",
      "Mean F1 Score: 0.6115\n",
      "Mean Sensitivity: 0.5167\n",
      "Mean Specificity: 0.8324\n",
      "\n",
      "\n",
      "Performance Metrics for task_14:\n",
      "Mean Accuracy: 0.5943\n",
      "Mean Precision: 0.5885\n",
      "Mean Recall: 0.8333\n",
      "Mean F1 Score: 0.6717\n",
      "Mean Sensitivity: 0.8333\n",
      "Mean Specificity: 0.3412\n",
      "\n",
      "\n",
      "Performance Metrics for task_15:\n",
      "Mean Accuracy: 0.7257\n",
      "Mean Precision: 0.8670\n",
      "Mean Recall: 0.5583\n",
      "Mean F1 Score: 0.6707\n",
      "Mean Sensitivity: 0.5583\n",
      "Mean Specificity: 0.9029\n",
      "\n",
      "\n",
      "Performance Metrics for task_16:\n",
      "Mean Accuracy: 0.7314\n",
      "Mean Precision: 0.8242\n",
      "Mean Recall: 0.6139\n",
      "Mean F1 Score: 0.6975\n",
      "Mean Sensitivity: 0.6139\n",
      "Mean Specificity: 0.8559\n",
      "\n",
      "\n",
      "Performance Metrics for task_17:\n",
      "Mean Accuracy: 0.7071\n",
      "Mean Precision: 0.7889\n",
      "Mean Recall: 0.6028\n",
      "Mean F1 Score: 0.6748\n",
      "Mean Sensitivity: 0.6028\n",
      "Mean Specificity: 0.8176\n",
      "\n",
      "\n",
      "Performance Metrics for task_18:\n",
      "Mean Accuracy: 0.5743\n",
      "Mean Precision: 0.5763\n",
      "Mean Recall: 0.8306\n",
      "Mean F1 Score: 0.6607\n",
      "Mean Sensitivity: 0.8306\n",
      "Mean Specificity: 0.3029\n",
      "\n",
      "\n",
      "Performance Metrics for task_19:\n",
      "Mean Accuracy: 0.5571\n",
      "Mean Precision: 0.5389\n",
      "Mean Recall: 0.9806\n",
      "Mean F1 Score: 0.6952\n",
      "Mean Sensitivity: 0.9806\n",
      "Mean Specificity: 0.1088\n",
      "\n",
      "\n",
      "Performance Metrics for task_20:\n",
      "Mean Accuracy: 0.6771\n",
      "Mean Precision: 0.8916\n",
      "Mean Recall: 0.4361\n",
      "Mean F1 Score: 0.5720\n",
      "Mean Sensitivity: 0.4361\n",
      "Mean Specificity: 0.9324\n",
      "\n",
      "\n",
      "Performance Metrics for task_21:\n",
      "Mean Accuracy: 0.6729\n",
      "Mean Precision: 0.8355\n",
      "Mean Recall: 0.4667\n",
      "Mean F1 Score: 0.5881\n",
      "Mean Sensitivity: 0.4667\n",
      "Mean Specificity: 0.8912\n",
      "\n",
      "\n",
      "Performance Metrics for task_22:\n",
      "Mean Accuracy: 0.6714\n",
      "Mean Precision: 0.8650\n",
      "Mean Recall: 0.4361\n",
      "Mean F1 Score: 0.5683\n",
      "Mean Sensitivity: 0.4361\n",
      "Mean Specificity: 0.9206\n",
      "\n",
      "\n",
      "Performance Metrics for task_23:\n",
      "Mean Accuracy: 0.7114\n",
      "Mean Precision: 0.7637\n",
      "Mean Recall: 0.6500\n",
      "Mean F1 Score: 0.6954\n",
      "Mean Sensitivity: 0.6500\n",
      "Mean Specificity: 0.7765\n",
      "\n",
      "\n",
      "Performance Metrics for task_24:\n",
      "Mean Accuracy: 0.6729\n",
      "Mean Precision: 0.7988\n",
      "Mean Recall: 0.4944\n",
      "Mean F1 Score: 0.6063\n",
      "Mean Sensitivity: 0.4944\n",
      "Mean Specificity: 0.8618\n",
      "\n",
      "\n",
      "Performance Metrics for task_25:\n",
      "Mean Accuracy: 0.6557\n",
      "Mean Precision: 0.7849\n",
      "Mean Recall: 0.4528\n",
      "Mean F1 Score: 0.5680\n",
      "Mean Sensitivity: 0.4528\n",
      "Mean Specificity: 0.8706\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "performance evaluation GAUSSIAN\n",
    "'''\n",
    " \n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    " \n",
    "# Number of runs\n",
    "n_runs = 20\n",
    " \n",
    "# Dictionary to store performance metrics for each task\n",
    "performance_metrics = {}\n",
    " \n",
    "# Iterate through tasks\n",
    "for task, task_df in task_dfs.items():\n",
    "   X_task = task_df\n",
    "   y_task = task_labels[task]\n",
    "  \n",
    "  \n",
    "   accuracies = []\n",
    "   precisions = []\n",
    "   recalls = []\n",
    "   f1_scores = []\n",
    "   sensitivities = []\n",
    "   specificities = []\n",
    " \n",
    "   for run in range(n_runs):\n",
    "       # Split the data\n",
    "       X_train, X_test, y_train, y_test = train_test_split(X_task, y_task, test_size=0.2, random_state=None, stratify=y_task)\n",
    "      \n",
    "       # Create classifier with best parameters\n",
    "       clf = GaussianNB(var_smoothing=1e-09)\n",
    "\n",
    "       y_train = y_train.values.ravel()\n",
    "       y_test = y_test.values.ravel()\n",
    "      \n",
    "       # Train the model\n",
    "       clf.fit(X_train, y_train)\n",
    "      \n",
    "       # Predict on test data\n",
    "       y_pred = clf.predict(X_test)\n",
    "      \n",
    "       # Calculate metrics\n",
    "       accuracy = accuracy_score(y_test, y_pred)\n",
    "       precision = precision_score(y_test, y_pred, pos_label='P')\n",
    "       recall = recall_score(y_test, y_pred, pos_label='P')\n",
    "       f1 = f1_score(y_test, y_pred, pos_label='P')\n",
    "      \n",
    "       # Compute confusion matrix\n",
    "       cm = confusion_matrix(y_test, y_pred, labels=['H', 'P'])\n",
    "       tn, fp, fn, tp = cm.ravel()\n",
    "      \n",
    "       # Calculate sensitivity and specificity\n",
    "       sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "       specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "      \n",
    "       # Append metrics\n",
    "       accuracies.append(accuracy)\n",
    "       precisions.append(precision)\n",
    "       recalls.append(recall)\n",
    "       f1_scores.append(f1)\n",
    "       sensitivities.append(sensitivity)\n",
    "       specificities.append(specificity)\n",
    "  \n",
    "   # Calculate average metrics\n",
    "   performance_metrics[task] = {\n",
    "       'mean_accuracy': np.mean(accuracies),\n",
    "       'mean_precision': np.mean(precisions),\n",
    "       'mean_recall': np.mean(recalls),\n",
    "       'mean_f1_score': np.mean(f1_scores),\n",
    "       'mean_sensitivity': np.mean(sensitivities),\n",
    "       'mean_specificity': np.mean(specificities)\n",
    "   }\n",
    " \n",
    "   print(f\"Performance Metrics for {task}:\")\n",
    "   print(f\"Mean Accuracy: {performance_metrics[task]['mean_accuracy']:.4f}\")\n",
    "   print(f\"Mean Precision: {performance_metrics[task]['mean_precision']:.4f}\")\n",
    "   print(f\"Mean Recall: {performance_metrics[task]['mean_recall']:.4f}\")\n",
    "   print(f\"Mean F1 Score: {performance_metrics[task]['mean_f1_score']:.4f}\")\n",
    "   print(f\"Mean Sensitivity: {performance_metrics[task]['mean_sensitivity']:.4f}\")\n",
    "   print(f\"Mean Specificity: {performance_metrics[task]['mean_specificity']:.4f}\")\n",
    "   print(\"\\n\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b41a7cc-c22d-4a2d-bbc8-c4a5eff2bf98",
   "metadata": {},
   "source": [
    "## Performance Evaluation of the GN Bayes classifier, using the k-fold cross-validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7a5c262-c375-49ec-8f4a-522e0874f5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Metrics for task_1:\n",
      "Mean Accuracy: 0.6091\n",
      "Mean Precision: 0.6757\n",
      "Mean Recall: 0.5217\n",
      "Mean F1 Score: 0.5594\n",
      "Mean Sensitivity: 0.5217\n",
      "Mean Specificity: 0.7063\n",
      "\n",
      "\n",
      "Performance Metrics for task_2:\n",
      "Mean Accuracy: 0.6899\n",
      "Mean Precision: 0.8395\n",
      "Mean Recall: 0.4739\n",
      "Mean F1 Score: 0.5942\n",
      "Mean Sensitivity: 0.4739\n",
      "Mean Specificity: 0.8969\n",
      "\n",
      "\n",
      "Performance Metrics for task_3:\n",
      "Mean Accuracy: 0.7067\n",
      "Mean Precision: 0.8346\n",
      "Mean Recall: 0.5349\n",
      "Mean F1 Score: 0.6458\n",
      "Mean Sensitivity: 0.5349\n",
      "Mean Specificity: 0.8801\n",
      "\n",
      "\n",
      "Performance Metrics for task_4:\n",
      "Mean Accuracy: 0.7066\n",
      "Mean Precision: 0.8517\n",
      "Mean Recall: 0.5281\n",
      "Mean F1 Score: 0.6472\n",
      "Mean Sensitivity: 0.5281\n",
      "Mean Specificity: 0.9070\n",
      "\n",
      "\n",
      "Performance Metrics for task_5:\n",
      "Mean Accuracy: 0.7296\n",
      "Mean Precision: 0.8971\n",
      "Mean Recall: 0.5360\n",
      "Mean F1 Score: 0.6645\n",
      "Mean Sensitivity: 0.5360\n",
      "Mean Specificity: 0.9331\n",
      "\n",
      "\n",
      "Performance Metrics for task_6:\n",
      "Mean Accuracy: 0.7188\n",
      "Mean Precision: 0.8843\n",
      "Mean Recall: 0.5308\n",
      "Mean F1 Score: 0.6563\n",
      "Mean Sensitivity: 0.5308\n",
      "Mean Specificity: 0.9226\n",
      "\n",
      "\n",
      "Performance Metrics for task_7:\n",
      "Mean Accuracy: 0.7126\n",
      "Mean Precision: 0.7452\n",
      "Mean Recall: 0.7131\n",
      "Mean F1 Score: 0.7146\n",
      "Mean Sensitivity: 0.7131\n",
      "Mean Specificity: 0.7202\n",
      "\n",
      "\n",
      "Performance Metrics for task_8:\n",
      "Mean Accuracy: 0.7119\n",
      "Mean Precision: 0.8468\n",
      "Mean Recall: 0.5149\n",
      "Mean F1 Score: 0.6365\n",
      "Mean Sensitivity: 0.5149\n",
      "Mean Specificity: 0.9006\n",
      "\n",
      "\n",
      "Performance Metrics for task_9:\n",
      "Mean Accuracy: 0.7412\n",
      "Mean Precision: 0.8607\n",
      "Mean Recall: 0.5595\n",
      "Mean F1 Score: 0.6716\n",
      "Mean Sensitivity: 0.5595\n",
      "Mean Specificity: 0.9168\n",
      "\n",
      "\n",
      "Performance Metrics for task_10:\n",
      "Mean Accuracy: 0.6899\n",
      "Mean Precision: 0.8559\n",
      "Mean Recall: 0.5003\n",
      "Mean F1 Score: 0.6112\n",
      "Mean Sensitivity: 0.5003\n",
      "Mean Specificity: 0.8923\n",
      "\n",
      "\n",
      "Performance Metrics for task_11:\n",
      "Mean Accuracy: 0.6555\n",
      "Mean Precision: 0.6276\n",
      "Mean Recall: 0.8348\n",
      "Mean F1 Score: 0.7097\n",
      "Mean Sensitivity: 0.8348\n",
      "Mean Specificity: 0.4694\n",
      "\n",
      "\n",
      "Performance Metrics for task_12:\n",
      "Mean Accuracy: 0.6035\n",
      "Mean Precision: 0.6979\n",
      "Mean Recall: 0.5109\n",
      "Mean F1 Score: 0.5527\n",
      "Mean Sensitivity: 0.5109\n",
      "Mean Specificity: 0.7264\n",
      "\n",
      "\n",
      "Performance Metrics for task_13:\n",
      "Mean Accuracy: 0.6956\n",
      "Mean Precision: 0.7964\n",
      "Mean Recall: 0.5336\n",
      "Mean F1 Score: 0.6358\n",
      "Mean Sensitivity: 0.5336\n",
      "Mean Specificity: 0.8645\n",
      "\n",
      "\n",
      "Performance Metrics for task_14:\n",
      "Mean Accuracy: 0.5926\n",
      "Mean Precision: 0.5978\n",
      "Mean Recall: 0.8439\n",
      "Mean F1 Score: 0.6766\n",
      "Mean Sensitivity: 0.8439\n",
      "Mean Specificity: 0.3690\n",
      "\n",
      "\n",
      "Performance Metrics for task_15:\n",
      "Mean Accuracy: 0.7185\n",
      "Mean Precision: 0.8586\n",
      "Mean Recall: 0.5433\n",
      "Mean F1 Score: 0.6573\n",
      "Mean Sensitivity: 0.5433\n",
      "Mean Specificity: 0.9088\n",
      "\n",
      "\n",
      "Performance Metrics for task_16:\n",
      "Mean Accuracy: 0.7365\n",
      "Mean Precision: 0.8227\n",
      "Mean Recall: 0.6082\n",
      "Mean F1 Score: 0.6911\n",
      "Mean Sensitivity: 0.6082\n",
      "Mean Specificity: 0.8759\n",
      "\n",
      "\n",
      "Performance Metrics for task_17:\n",
      "Mean Accuracy: 0.7071\n",
      "Mean Precision: 0.7992\n",
      "Mean Recall: 0.5980\n",
      "Mean F1 Score: 0.6716\n",
      "Mean Sensitivity: 0.5980\n",
      "Mean Specificity: 0.8387\n",
      "\n",
      "\n",
      "Performance Metrics for task_18:\n",
      "Mean Accuracy: 0.5624\n",
      "Mean Precision: 0.5600\n",
      "Mean Recall: 0.8812\n",
      "Mean F1 Score: 0.6645\n",
      "Mean Sensitivity: 0.8812\n",
      "Mean Specificity: 0.2694\n",
      "\n",
      "\n",
      "Performance Metrics for task_19:\n",
      "Mean Accuracy: 0.5511\n",
      "Mean Precision: 0.5301\n",
      "Mean Recall: 0.9526\n",
      "Mean F1 Score: 0.6719\n",
      "Mean Sensitivity: 0.9526\n",
      "Mean Specificity: 0.1265\n",
      "\n",
      "\n",
      "Performance Metrics for task_20:\n",
      "Mean Accuracy: 0.6600\n",
      "Mean Precision: 0.8528\n",
      "Mean Recall: 0.4067\n",
      "Mean F1 Score: 0.5459\n",
      "Mean Sensitivity: 0.4067\n",
      "Mean Specificity: 0.9249\n",
      "\n",
      "\n",
      "Performance Metrics for task_21:\n",
      "Mean Accuracy: 0.6721\n",
      "Mean Precision: 0.8112\n",
      "Mean Recall: 0.4554\n",
      "Mean F1 Score: 0.5759\n",
      "Mean Sensitivity: 0.4554\n",
      "Mean Specificity: 0.8944\n",
      "\n",
      "\n",
      "Performance Metrics for task_22:\n",
      "Mean Accuracy: 0.6835\n",
      "Mean Precision: 0.9200\n",
      "Mean Recall: 0.4151\n",
      "Mean F1 Score: 0.5510\n",
      "Mean Sensitivity: 0.4151\n",
      "Mean Specificity: 0.9636\n",
      "\n",
      "\n",
      "Performance Metrics for task_23:\n",
      "Mean Accuracy: 0.6837\n",
      "Mean Precision: 0.7350\n",
      "Mean Recall: 0.6000\n",
      "Mean F1 Score: 0.6519\n",
      "Mean Sensitivity: 0.6000\n",
      "Mean Specificity: 0.7748\n",
      "\n",
      "\n",
      "Performance Metrics for task_24:\n",
      "Mean Accuracy: 0.6666\n",
      "Mean Precision: 0.7855\n",
      "Mean Recall: 0.4955\n",
      "Mean F1 Score: 0.6035\n",
      "Mean Sensitivity: 0.4955\n",
      "Mean Specificity: 0.8447\n",
      "\n",
      "\n",
      "Performance Metrics for task_25:\n",
      "Mean Accuracy: 0.6728\n",
      "Mean Precision: 0.8314\n",
      "Mean Recall: 0.4626\n",
      "Mean F1 Score: 0.5833\n",
      "Mean Sensitivity: 0.4626\n",
      "Mean Specificity: 0.8968\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Performance evaluation GAUSSIAN with K-Fold cross-validation\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Number of folds\n",
    "n_splits = 5  # You can adjust the number of folds as needed\n",
    "\n",
    "# Dictionary to store performance metrics for each task\n",
    "performance_metrics = {}\n",
    "\n",
    "# Iterate through tasks\n",
    "for task, task_df in task_dfs.items():\n",
    "    X_task = task_df\n",
    "    y_task = task_labels[task]\n",
    "\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    sensitivities = []\n",
    "    specificities = []\n",
    "\n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=None)\n",
    "\n",
    "    for train_index, test_index in kf.split(X_task):\n",
    "        X_train, X_test = X_task.iloc[train_index], X_task.iloc[test_index]\n",
    "        y_train, y_test = y_task.iloc[train_index], y_task.iloc[test_index]\n",
    "\n",
    "        # Create classifier with best parameters\n",
    "        clf = GaussianNB(var_smoothing=1e-09)\n",
    "\n",
    "        y_train = y_train.values.ravel()\n",
    "        y_test = y_test.values.ravel()\n",
    "\n",
    "        # Train the model\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on test data\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, pos_label='P')\n",
    "        recall = recall_score(y_test, y_pred, pos_label='P')\n",
    "        f1 = f1_score(y_test, y_pred, pos_label='P')\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=['H', 'P'])\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        # Calculate sensitivity and specificity\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "\n",
    "        # Append metrics\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        sensitivities.append(sensitivity)\n",
    "        specificities.append(specificity)\n",
    "\n",
    "    # Calculate average metrics\n",
    "    performance_metrics[task] = {\n",
    "        'mean_accuracy': np.mean(accuracies),\n",
    "        'mean_precision': np.mean(precisions),\n",
    "        'mean_recall': np.mean(recalls),\n",
    "        'mean_f1_score': np.mean(f1_scores),\n",
    "        'mean_sensitivity': np.mean(sensitivities),\n",
    "        'mean_specificity': np.mean(specificities)\n",
    "    }\n",
    "\n",
    "    print(f\"Performance Metrics for {task}:\")\n",
    "    print(f\"Mean Accuracy: {performance_metrics[task]['mean_accuracy']:.4f}\")\n",
    "    print(f\"Mean Precision: {performance_metrics[task]['mean_precision']:.4f}\")\n",
    "    print(f\"Mean Recall: {performance_metrics[task]['mean_recall']:.4f}\")\n",
    "    print(f\"Mean F1 Score: {performance_metrics[task]['mean_f1_score']:.4f}\")\n",
    "    print(f\"Mean Sensitivity: {performance_metrics[task]['mean_sensitivity']:.4f}\")\n",
    "    print(f\"Mean Specificity: {performance_metrics[task]['mean_specificity']:.4f}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be352f84-c09d-49de-b3cb-2fec42b674a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
