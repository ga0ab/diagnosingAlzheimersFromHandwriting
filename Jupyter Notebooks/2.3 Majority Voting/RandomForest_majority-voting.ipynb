{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec709f90-836b-4965-88fe-4e28b08b450f",
   "metadata": {},
   "source": [
    "## For each of our four classification models, we created an ensemble by **combining the predictions of all 25 taskspecific classifiers**. This resulted in four model-specific ensembles, each integrating information from all handwriting tasks within a single classification framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db522364-b3c5-4ef7-a3a9-b6b55f81206e",
   "metadata": {},
   "source": [
    "## Let's do this for **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdf2a982-8689-4701-ad3a-038d6625bf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metrics after 20 runs:\n",
      "Mean Accuracy: 0.8729\n",
      "Mean Precision: 0.8384\n",
      "Mean Recall: 0.9176\n",
      "Mean F1 Score: 0.8747\n",
      "Mean Sensitivity: 0.9176\n",
      "Mean Specificity: 0.8306\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Fetch dataset\n",
    "darwin = fetch_ucirepo(id=732)\n",
    "\n",
    "# Data (as pandas dataframes)\n",
    "X = darwin.data.features\n",
    "y = darwin.data.targets\n",
    "\n",
    "# Drop the 'ID' column if it exists\n",
    "if 'ID' in X.columns:\n",
    "    X = X.drop(columns=['ID'])\n",
    "\n",
    "# Ensure y is a Series\n",
    "if isinstance(y, pd.DataFrame):\n",
    "    y = y.iloc[:, 0]\n",
    "\n",
    "# Number of attributes per task and number of tasks\n",
    "num_attributes_per_task = 18\n",
    "num_tasks = 25\n",
    "\n",
    "# Define the number of runs\n",
    "n_runs = 20\n",
    "\n",
    "# Initialize lists to store the metrics\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "specificities = []\n",
    "sensitivities = []\n",
    "\n",
    "# Convert class labels to integers for processing\n",
    "class_mapping = {label: idx for idx, label in enumerate(y.unique())}\n",
    "reverse_class_mapping = {idx: label for label, idx in class_mapping.items()}\n",
    "\n",
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'bootstrap': True,\n",
    "    'criterion': 'gini',\n",
    "    'max_depth': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_samples_split': 2,\n",
    "    'n_estimators': 50\n",
    "}\n",
    "\n",
    "for run in range(n_runs):\n",
    "    # Shuffle and split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=None, stratify=y)\n",
    "\n",
    "    # Ensure there are enough features for the tasks\n",
    "    if X_train.shape[1] < num_attributes_per_task * num_tasks:\n",
    "        raise ValueError(\"Not enough features in X_train for the specified number of tasks and attributes per task.\")\n",
    "\n",
    "    # Store predictions from each task\n",
    "    task_predictions = []\n",
    "\n",
    "    # Train a Random Forest classifier for each task\n",
    "    for i in range(num_tasks):\n",
    "        # Get feature columns for the current task\n",
    "        start_index = i * num_attributes_per_task\n",
    "        end_index = start_index + num_attributes_per_task\n",
    "        task_columns = X_train.columns[start_index:end_index]\n",
    "\n",
    "        # Train the Random Forest classifier\n",
    "        clf = RandomForestClassifier(\n",
    "            bootstrap=rf_params['bootstrap'],\n",
    "            criterion=rf_params['criterion'],\n",
    "            max_depth=rf_params['max_depth'],\n",
    "            min_samples_leaf=rf_params['min_samples_leaf'],\n",
    "            min_samples_split=rf_params['min_samples_split'],\n",
    "            n_estimators=rf_params['n_estimators'],\n",
    "            random_state=None\n",
    "        )\n",
    "\n",
    "        clf.fit(X_train[task_columns], y_train)\n",
    "\n",
    "        # Predict on the test set and convert to integer labels\n",
    "        task_pred = clf.predict(X_test[task_columns])\n",
    "        task_pred_int = np.array([class_mapping[pred] for pred in task_pred])\n",
    "        task_predictions.append(task_pred_int)\n",
    "\n",
    "    # Convert task_predictions to a numpy array for easier manipulation\n",
    "    task_predictions = np.array(task_predictions)\n",
    "\n",
    "    # Majority voting: for each instance, take the most common prediction across all tasks\n",
    "    final_predictions_int = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=task_predictions)\n",
    "\n",
    "    # Convert integer labels back to original class names\n",
    "    final_predictions = np.array([reverse_class_mapping[pred] for pred in final_predictions_int])\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracies.append(accuracy_score(y_test, final_predictions))\n",
    "    precisions.append(precision_score(y_test, final_predictions, pos_label=list(class_mapping.keys())[1]))\n",
    "    recalls.append(recall_score(y_test, final_predictions, pos_label=list(class_mapping.keys())[1]))\n",
    "    f1_scores.append(f1_score(y_test, final_predictions, pos_label=list(class_mapping.keys())[1]))\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, final_predictions, labels=list(class_mapping.keys()))\n",
    "\n",
    "    # Extract confusion matrix components for the positive class\n",
    "    TP = cm[1, 1]\n",
    "    FN = cm[1, 0]\n",
    "    FP = cm[0, 1]\n",
    "    TN = cm[0, 0]\n",
    "\n",
    "    # Calculate sensitivity (True Positive Rate)\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    sensitivities.append(sensitivity)\n",
    "\n",
    "    # Calculate specificity (True Negative Rate)\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    specificities.append(specificity)\n",
    "\n",
    "# Calculate the mean of the metrics\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "mean_precision = np.mean(precisions)\n",
    "mean_recall = np.mean(recalls)\n",
    "mean_f1_score = np.mean(f1_scores)\n",
    "mean_sensitivity = np.mean(sensitivities)\n",
    "mean_specificity = np.mean(specificities)\n",
    "\n",
    "# Print the average metrics after 20 runs\n",
    "print(\"Average Metrics after 20 runs:\")\n",
    "print(f\"Mean Accuracy: {mean_accuracy:.4f}\")\n",
    "print(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "print(f\"Mean Recall: {mean_recall:.4f}\")\n",
    "print(f\"Mean F1 Score: {mean_f1_score:.4f}\")\n",
    "print(f\"Mean Sensitivity: {mean_sensitivity:.4f}\")\n",
    "print(f\"Mean Specificity: {mean_specificity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c319ebdd-4d75-4c14-b8ae-daa6cf7f42fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
