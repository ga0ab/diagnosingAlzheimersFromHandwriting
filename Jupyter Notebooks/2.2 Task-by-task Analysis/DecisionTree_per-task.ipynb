{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a75b9a-922d-4df1-91d6-04c03805f48f",
   "metadata": {},
   "source": [
    "## This approach involved training and evaluating our four classifiers on 25 distinct feature sets, each corresponding to a specific handwriting task in the DARWIN data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74736214-c1e4-4557-8551-4d768197abb9",
   "metadata": {},
   "source": [
    "### Here we do it for the Decision Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "700451d5-c2d7-4f7b-98e6-6af7e5077a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We extract the feature vectors from each of the 25 tasks.\n",
    "'''\n",
    " \n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    " \n",
    "# Fetch dataset \n",
    "darwin = fetch_ucirepo(id=732)\n",
    " \n",
    "# Data (as pandas dataframes)\n",
    "X = darwin.data.features\n",
    "y = darwin.data.targets\n",
    " \n",
    "X = X.drop(columns=['ID'])\n",
    " \n",
    "# Number of attributes per task\n",
    "num_attributes_per_task = 18\n",
    " \n",
    "# Number of tasks\n",
    "num_tasks = 25\n",
    " \n",
    "# Create a dictionary to hold the DataFrames for each task\n",
    "task_dfs = {}\n",
    " \n",
    "# Create a dictionary to hold the labels for each task\n",
    "task_labels = {}\n",
    " \n",
    "# Iterate through the number of tasks\n",
    "for i in range(num_tasks):\n",
    "    # Column indices for the current task\n",
    "    start_index = i * num_attributes_per_task\n",
    "    end_index = start_index + num_attributes_per_task\n",
    "    # Select columns for the current task\n",
    "    task_columns = X.columns[start_index:end_index]\n",
    "    # Create a DataFrame for the current task\n",
    "    task_df = X[task_columns].copy()\n",
    "    # Store the DataFrame in the dictionary with the key 'task_i'\n",
    "    task_dfs[f'task_{i + 1}'] = task_df\n",
    "    # Select labels for the current task\n",
    "    task_labels[f'task_{i + 1}'] = y.copy()  # Labels are identical for all tasks, adjust if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508cbdaa-8341-4dc5-be2a-a49d3d2f5b06",
   "metadata": {},
   "source": [
    "### Performing grid search to compute the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07c4d735-b91c-4746-ad29-522fdb3235e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_1: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_2: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_3: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_4: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_5: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_6: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_7: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_8: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_9: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_10: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_11: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_12: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_13: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_14: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_15: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_16: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_17: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_18: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_19: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_20: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_21: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_22: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_23: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for task_24: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n",
      "Best Parameters for task_25: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\anaconda3\\envs\\DMML\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "grid search\n",
    "'''\n",
    " \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    " \n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "   'criterion': ['gini', 'entropy'],\n",
    "   'max_depth': list(range(2, 11)),\n",
    "   'min_samples_split': list(range(2, 6)),\n",
    "   'min_samples_leaf': list(range(2, 21, 2)),\n",
    "   'max_leaf_nodes': list(range(2, 21, 2))\n",
    "}\n",
    " \n",
    "# Dictionary to store the best parameters for each task\n",
    "best_params_per_task = {}\n",
    " \n",
    "# Iterate through tasks\n",
    "for task, task_df in task_dfs.items():\n",
    "   X_task = task_df\n",
    "   y_task = task_labels[task]\n",
    " \n",
    "   # Split the data\n",
    "   X_train, X_test, y_train, y_test = train_test_split(X_task, y_task, test_size=0.2, random_state=42, stratify=y_task)\n",
    " \n",
    "   # Initialize Decision Tree Classifier\n",
    "   clf = DecisionTreeClassifier(random_state=42)\n",
    " \n",
    "   # Grid Search\n",
    "   grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1, scoring='recall')\n",
    "   grid_search.fit(X_train, y_train)\n",
    " \n",
    "   # Store the best parameters for this task\n",
    "   best_params_per_task[task] = grid_search.best_params_\n",
    " \n",
    "   print(f\"Best Parameters for {task}: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4a3228-fa42-41c4-9fdc-2c01af999ac0",
   "metadata": {},
   "source": [
    "## The results given by the grid-search show that the optimal parameters are identical for each of the 25 tasks:\n",
    "\n",
    "**best_parameters: {'criterion': 'gini', 'max_depth': 2, 'max_leaf_nodes': 2, 'min_samples_leaf': 2, 'min_samples_split': 2}**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b3a05a-f73f-4905-b20e-2506fa90942f",
   "metadata": {},
   "source": [
    "## Performance Evaluation of Decision Tree classifier, using the 20 runs method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92023a10-2cbb-45c1-b279-86a063d15527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Metrics for task_1:\n",
      "Mean Accuracy: 0.5871\n",
      "Mean Precision: 0.6840\n",
      "Mean Recall: 0.4000\n",
      "Mean F1 Score: 0.4853\n",
      "Mean Sensitivity: 0.4000\n",
      "Mean Specificity: 0.7853\n",
      "\n",
      "\n",
      "Performance Metrics for task_2:\n",
      "Mean Accuracy: 0.6671\n",
      "Mean Precision: 0.7383\n",
      "Mean Recall: 0.5889\n",
      "Mean F1 Score: 0.6304\n",
      "Mean Sensitivity: 0.5889\n",
      "Mean Specificity: 0.7500\n",
      "\n",
      "\n",
      "Performance Metrics for task_3:\n",
      "Mean Accuracy: 0.7171\n",
      "Mean Precision: 0.8564\n",
      "Mean Recall: 0.5389\n",
      "Mean F1 Score: 0.6566\n",
      "Mean Sensitivity: 0.5389\n",
      "Mean Specificity: 0.9059\n",
      "\n",
      "\n",
      "Performance Metrics for task_4:\n",
      "Mean Accuracy: 0.6443\n",
      "Mean Precision: 0.7198\n",
      "Mean Recall: 0.5306\n",
      "Mean F1 Score: 0.5875\n",
      "Mean Sensitivity: 0.5306\n",
      "Mean Specificity: 0.7647\n",
      "\n",
      "\n",
      "Performance Metrics for task_5:\n",
      "Mean Accuracy: 0.6757\n",
      "Mean Precision: 0.7169\n",
      "Mean Recall: 0.6389\n",
      "Mean F1 Score: 0.6628\n",
      "Mean Sensitivity: 0.6389\n",
      "Mean Specificity: 0.7147\n",
      "\n",
      "\n",
      "Performance Metrics for task_6:\n",
      "Mean Accuracy: 0.7343\n",
      "Mean Precision: 0.7857\n",
      "Mean Recall: 0.6667\n",
      "Mean F1 Score: 0.7148\n",
      "Mean Sensitivity: 0.6667\n",
      "Mean Specificity: 0.8059\n",
      "\n",
      "\n",
      "Performance Metrics for task_7:\n",
      "Mean Accuracy: 0.7186\n",
      "Mean Precision: 0.7538\n",
      "Mean Recall: 0.6917\n",
      "Mean F1 Score: 0.7102\n",
      "Mean Sensitivity: 0.6917\n",
      "Mean Specificity: 0.7471\n",
      "\n",
      "\n",
      "Performance Metrics for task_8:\n",
      "Mean Accuracy: 0.6900\n",
      "Mean Precision: 0.7716\n",
      "Mean Recall: 0.5778\n",
      "Mean F1 Score: 0.6506\n",
      "Mean Sensitivity: 0.5778\n",
      "Mean Specificity: 0.8088\n",
      "\n",
      "\n",
      "Performance Metrics for task_9:\n",
      "Mean Accuracy: 0.6957\n",
      "Mean Precision: 0.8467\n",
      "Mean Recall: 0.5028\n",
      "Mean F1 Score: 0.6241\n",
      "Mean Sensitivity: 0.5028\n",
      "Mean Specificity: 0.9000\n",
      "\n",
      "\n",
      "Performance Metrics for task_10:\n",
      "Mean Accuracy: 0.6014\n",
      "Mean Precision: 0.7342\n",
      "Mean Recall: 0.5111\n",
      "Mean F1 Score: 0.5381\n",
      "Mean Sensitivity: 0.5111\n",
      "Mean Specificity: 0.6971\n",
      "\n",
      "\n",
      "Performance Metrics for task_11:\n",
      "Mean Accuracy: 0.6286\n",
      "Mean Precision: 0.6352\n",
      "Mean Recall: 0.7333\n",
      "Mean F1 Score: 0.6568\n",
      "Mean Sensitivity: 0.7333\n",
      "Mean Specificity: 0.5176\n",
      "\n",
      "\n",
      "Performance Metrics for task_12:\n",
      "Mean Accuracy: 0.6629\n",
      "Mean Precision: 0.8199\n",
      "Mean Recall: 0.4611\n",
      "Mean F1 Score: 0.5683\n",
      "Mean Sensitivity: 0.4611\n",
      "Mean Specificity: 0.8765\n",
      "\n",
      "\n",
      "Performance Metrics for task_13:\n",
      "Mean Accuracy: 0.6686\n",
      "Mean Precision: 0.7420\n",
      "Mean Recall: 0.5583\n",
      "Mean F1 Score: 0.6260\n",
      "Mean Sensitivity: 0.5583\n",
      "Mean Specificity: 0.7853\n",
      "\n",
      "\n",
      "Performance Metrics for task_14:\n",
      "Mean Accuracy: 0.5800\n",
      "Mean Precision: 0.7111\n",
      "Mean Recall: 0.3750\n",
      "Mean F1 Score: 0.4653\n",
      "Mean Sensitivity: 0.3750\n",
      "Mean Specificity: 0.7971\n",
      "\n",
      "\n",
      "Performance Metrics for task_15:\n",
      "Mean Accuracy: 0.7414\n",
      "Mean Precision: 0.8266\n",
      "Mean Recall: 0.6361\n",
      "Mean F1 Score: 0.7099\n",
      "Mean Sensitivity: 0.6361\n",
      "Mean Specificity: 0.8529\n",
      "\n",
      "\n",
      "Performance Metrics for task_16:\n",
      "Mean Accuracy: 0.7357\n",
      "Mean Precision: 0.8607\n",
      "Mean Recall: 0.5861\n",
      "Mean F1 Score: 0.6905\n",
      "Mean Sensitivity: 0.5861\n",
      "Mean Specificity: 0.8941\n",
      "\n",
      "\n",
      "Performance Metrics for task_17:\n",
      "Mean Accuracy: 0.7157\n",
      "Mean Precision: 0.7751\n",
      "Mean Recall: 0.6611\n",
      "Mean F1 Score: 0.7009\n",
      "Mean Sensitivity: 0.6611\n",
      "Mean Specificity: 0.7735\n",
      "\n",
      "\n",
      "Performance Metrics for task_18:\n",
      "Mean Accuracy: 0.6300\n",
      "Mean Precision: 0.6733\n",
      "Mean Recall: 0.5722\n",
      "Mean F1 Score: 0.5910\n",
      "Mean Sensitivity: 0.5722\n",
      "Mean Specificity: 0.6912\n",
      "\n",
      "\n",
      "Performance Metrics for task_19:\n",
      "Mean Accuracy: 0.6814\n",
      "Mean Precision: 0.8350\n",
      "Mean Recall: 0.4917\n",
      "Mean F1 Score: 0.6086\n",
      "Mean Sensitivity: 0.4917\n",
      "Mean Specificity: 0.8824\n",
      "\n",
      "\n",
      "Performance Metrics for task_20:\n",
      "Mean Accuracy: 0.6671\n",
      "Mean Precision: 0.6816\n",
      "Mean Recall: 0.7083\n",
      "Mean F1 Score: 0.6832\n",
      "Mean Sensitivity: 0.7083\n",
      "Mean Specificity: 0.6235\n",
      "\n",
      "\n",
      "Performance Metrics for task_21:\n",
      "Mean Accuracy: 0.6286\n",
      "Mean Precision: 0.6307\n",
      "Mean Recall: 0.7111\n",
      "Mean F1 Score: 0.6635\n",
      "Mean Sensitivity: 0.7111\n",
      "Mean Specificity: 0.5412\n",
      "\n",
      "\n",
      "Performance Metrics for task_22:\n",
      "Mean Accuracy: 0.6957\n",
      "Mean Precision: 0.7640\n",
      "Mean Recall: 0.6500\n",
      "Mean F1 Score: 0.6715\n",
      "Mean Sensitivity: 0.6500\n",
      "Mean Specificity: 0.7441\n",
      "\n",
      "\n",
      "Performance Metrics for task_23:\n",
      "Mean Accuracy: 0.8543\n",
      "Mean Precision: 0.8236\n",
      "Mean Recall: 0.9194\n",
      "Mean F1 Score: 0.8673\n",
      "Mean Sensitivity: 0.9194\n",
      "Mean Specificity: 0.7853\n",
      "\n",
      "\n",
      "Performance Metrics for task_24:\n",
      "Mean Accuracy: 0.6500\n",
      "Mean Precision: 0.7530\n",
      "Mean Recall: 0.5722\n",
      "Mean F1 Score: 0.6036\n",
      "Mean Sensitivity: 0.5722\n",
      "Mean Specificity: 0.7324\n",
      "\n",
      "\n",
      "Performance Metrics for task_25:\n",
      "Mean Accuracy: 0.6786\n",
      "Mean Precision: 0.6652\n",
      "Mean Recall: 0.7722\n",
      "Mean F1 Score: 0.7125\n",
      "Mean Sensitivity: 0.7722\n",
      "Mean Specificity: 0.5794\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "performance evaluation using 20 run method\n",
    "'''\n",
    " \n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    " \n",
    "# Number of runs\n",
    "n_runs = 20\n",
    " \n",
    "# Dictionary to store performance metrics for each task\n",
    "performance_metrics = {}\n",
    "\n",
    "# Using the best parameters computed during grid-search\n",
    "best_params = {\n",
    "   'criterion': 'gini', \n",
    "   'max_depth': 2,\n",
    "   'max_leaf_nodes': 2, \n",
    "   'min_samples_leaf': 2, \n",
    "   'min_samples_split': 2\n",
    "}\n",
    " \n",
    "# Iterate through tasks\n",
    "for task, task_df in task_dfs.items():\n",
    "    X_task = task_df\n",
    "    y_task = task_labels[task]\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    sensitivities = []\n",
    "    specificities = []\n",
    " \n",
    "    for run in range(n_runs):\n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_task, y_task, test_size=0.2, random_state=None, stratify=y_task)\n",
    "        # Create classifier with best parameters\n",
    "        clf = DecisionTreeClassifier(**best_params)\n",
    "        # Train the model\n",
    "        clf.fit(X_train, y_train)\n",
    "        # Predict\n",
    "        y_pred = clf.predict(X_test)\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, pos_label='P')\n",
    "        recall = recall_score(y_test, y_pred, pos_label='P')\n",
    "        f1 = f1_score(y_test, y_pred, pos_label='P')\n",
    "        # Calculate confusion matrix components\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=['H', 'P'])\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "        # Append metrics\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        sensitivities.append(sensitivity)\n",
    "        specificities.append(specificity)\n",
    "    # Calculate average metrics\n",
    "    performance_metrics[task] = {\n",
    "        'mean_accuracy': np.mean(accuracies),\n",
    "        'mean_precision': np.mean(precisions),\n",
    "        'mean_recall': np.mean(recalls),\n",
    "        'mean_f1_score': np.mean(f1_scores),\n",
    "        'mean_sensitivity': np.mean(sensitivities),\n",
    "        'mean_specificity': np.mean(specificities)\n",
    "    }\n",
    " \n",
    "    print(f\"Performance Metrics for {task}:\")\n",
    "    print(f\"Mean Accuracy: {performance_metrics[task]['mean_accuracy']:.4f}\")\n",
    "    print(f\"Mean Precision: {performance_metrics[task]['mean_precision']:.4f}\")\n",
    "    print(f\"Mean Recall: {performance_metrics[task]['mean_recall']:.4f}\")\n",
    "    print(f\"Mean F1 Score: {performance_metrics[task]['mean_f1_score']:.4f}\")\n",
    "    print(f\"Mean Sensitivity: {performance_metrics[task]['mean_sensitivity']:.4f}\")\n",
    "    print(f\"Mean Specificity: {performance_metrics[task]['mean_specificity']:.4f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ab8b89-74b6-4141-a06c-cbab8b8348a4",
   "metadata": {},
   "source": [
    "## Performance Evaluation of Decision Tree classifier, using the k-fold cross-validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3a8bc85-f363-4670-937d-9835b2f94b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Metrics for task_1:\n",
      "Mean Accuracy: 0.5459\n",
      "Mean Precision: 0.6383\n",
      "Mean Recall: 0.3428\n",
      "Mean F1 Score: 0.4127\n",
      "Mean Sensitivity: 0.3428\n",
      "Mean Specificity: 0.7560\n",
      "\n",
      "\n",
      "Performance Metrics for task_2:\n",
      "Mean Accuracy: 0.6780\n",
      "Mean Precision: 0.7857\n",
      "Mean Recall: 0.5498\n",
      "Mean F1 Score: 0.6291\n",
      "Mean Sensitivity: 0.5498\n",
      "Mean Specificity: 0.8117\n",
      "\n",
      "\n",
      "Performance Metrics for task_3:\n",
      "Mean Accuracy: 0.7356\n",
      "Mean Precision: 0.8642\n",
      "Mean Recall: 0.5712\n",
      "Mean F1 Score: 0.6870\n",
      "Mean Sensitivity: 0.5712\n",
      "Mean Specificity: 0.9035\n",
      "\n",
      "\n",
      "Performance Metrics for task_4:\n",
      "Mean Accuracy: 0.6499\n",
      "Mean Precision: 0.6784\n",
      "Mean Recall: 0.6380\n",
      "Mean F1 Score: 0.6479\n",
      "Mean Sensitivity: 0.6380\n",
      "Mean Specificity: 0.6791\n",
      "\n",
      "\n",
      "Performance Metrics for task_5:\n",
      "Mean Accuracy: 0.6612\n",
      "Mean Precision: 0.6697\n",
      "Mean Recall: 0.6717\n",
      "Mean F1 Score: 0.6659\n",
      "Mean Sensitivity: 0.6717\n",
      "Mean Specificity: 0.6557\n",
      "\n",
      "\n",
      "Performance Metrics for task_6:\n",
      "Mean Accuracy: 0.7358\n",
      "Mean Precision: 0.8082\n",
      "Mean Recall: 0.6617\n",
      "Mean F1 Score: 0.7141\n",
      "Mean Sensitivity: 0.6617\n",
      "Mean Specificity: 0.8385\n",
      "\n",
      "\n",
      "Performance Metrics for task_7:\n",
      "Mean Accuracy: 0.7474\n",
      "Mean Precision: 0.8240\n",
      "Mean Recall: 0.6564\n",
      "Mean F1 Score: 0.7213\n",
      "Mean Sensitivity: 0.6564\n",
      "Mean Specificity: 0.8272\n",
      "\n",
      "\n",
      "Performance Metrics for task_8:\n",
      "Mean Accuracy: 0.6659\n",
      "Mean Precision: 0.7564\n",
      "Mean Recall: 0.5643\n",
      "Mean F1 Score: 0.6283\n",
      "Mean Sensitivity: 0.5643\n",
      "Mean Specificity: 0.7930\n",
      "\n",
      "\n",
      "Performance Metrics for task_9:\n",
      "Mean Accuracy: 0.6839\n",
      "Mean Precision: 0.8455\n",
      "Mean Recall: 0.4870\n",
      "Mean F1 Score: 0.6030\n",
      "Mean Sensitivity: 0.4870\n",
      "Mean Specificity: 0.8981\n",
      "\n",
      "\n",
      "Performance Metrics for task_10:\n",
      "Mean Accuracy: 0.5802\n",
      "Mean Precision: 0.6938\n",
      "Mean Recall: 0.4980\n",
      "Mean F1 Score: 0.5181\n",
      "Mean Sensitivity: 0.4980\n",
      "Mean Specificity: 0.7488\n",
      "\n",
      "\n",
      "Performance Metrics for task_11:\n",
      "Mean Accuracy: 0.6037\n",
      "Mean Precision: 0.6150\n",
      "Mean Recall: 0.6967\n",
      "Mean F1 Score: 0.6160\n",
      "Mean Sensitivity: 0.6967\n",
      "Mean Specificity: 0.5513\n",
      "\n",
      "\n",
      "Performance Metrics for task_12:\n",
      "Mean Accuracy: 0.6551\n",
      "Mean Precision: 0.7845\n",
      "Mean Recall: 0.4285\n",
      "Mean F1 Score: 0.5517\n",
      "Mean Sensitivity: 0.4285\n",
      "Mean Specificity: 0.8866\n",
      "\n",
      "\n",
      "Performance Metrics for task_13:\n",
      "Mean Accuracy: 0.6901\n",
      "Mean Precision: 0.7949\n",
      "Mean Recall: 0.5670\n",
      "Mean F1 Score: 0.6462\n",
      "Mean Sensitivity: 0.5670\n",
      "Mean Specificity: 0.8274\n",
      "\n",
      "\n",
      "Performance Metrics for task_14:\n",
      "Mean Accuracy: 0.5976\n",
      "Mean Precision: 0.6538\n",
      "Mean Recall: 0.5085\n",
      "Mean F1 Score: 0.5451\n",
      "Mean Sensitivity: 0.5085\n",
      "Mean Specificity: 0.6774\n",
      "\n",
      "\n",
      "Performance Metrics for task_15:\n",
      "Mean Accuracy: 0.7696\n",
      "Mean Precision: 0.8620\n",
      "Mean Recall: 0.6706\n",
      "Mean F1 Score: 0.7442\n",
      "Mean Sensitivity: 0.6706\n",
      "Mean Specificity: 0.8608\n",
      "\n",
      "\n",
      "Performance Metrics for task_16:\n",
      "Mean Accuracy: 0.7418\n",
      "Mean Precision: 0.8689\n",
      "Mean Recall: 0.5998\n",
      "Mean F1 Score: 0.7011\n",
      "Mean Sensitivity: 0.5998\n",
      "Mean Specificity: 0.8914\n",
      "\n",
      "\n",
      "Performance Metrics for task_17:\n",
      "Mean Accuracy: 0.7355\n",
      "Mean Precision: 0.7801\n",
      "Mean Recall: 0.6735\n",
      "Mean F1 Score: 0.7197\n",
      "Mean Sensitivity: 0.6735\n",
      "Mean Specificity: 0.7966\n",
      "\n",
      "\n",
      "Performance Metrics for task_18:\n",
      "Mean Accuracy: 0.6153\n",
      "Mean Precision: 0.6360\n",
      "Mean Recall: 0.6206\n",
      "Mean F1 Score: 0.5792\n",
      "Mean Sensitivity: 0.6206\n",
      "Mean Specificity: 0.6286\n",
      "\n",
      "\n",
      "Performance Metrics for task_19:\n",
      "Mean Accuracy: 0.7126\n",
      "Mean Precision: 0.8561\n",
      "Mean Recall: 0.5041\n",
      "Mean F1 Score: 0.6329\n",
      "Mean Sensitivity: 0.5041\n",
      "Mean Specificity: 0.9164\n",
      "\n",
      "\n",
      "Performance Metrics for task_20:\n",
      "Mean Accuracy: 0.6556\n",
      "Mean Precision: 0.6352\n",
      "Mean Recall: 0.6756\n",
      "Mean F1 Score: 0.6473\n",
      "Mean Sensitivity: 0.6756\n",
      "Mean Specificity: 0.6306\n",
      "\n",
      "\n",
      "Performance Metrics for task_21:\n",
      "Mean Accuracy: 0.6491\n",
      "Mean Precision: 0.6631\n",
      "Mean Recall: 0.7094\n",
      "Mean F1 Score: 0.6705\n",
      "Mean Sensitivity: 0.7094\n",
      "Mean Specificity: 0.6106\n",
      "\n",
      "\n",
      "Performance Metrics for task_22:\n",
      "Mean Accuracy: 0.6953\n",
      "Mean Precision: 0.7929\n",
      "Mean Recall: 0.5708\n",
      "Mean F1 Score: 0.6578\n",
      "Mean Sensitivity: 0.5708\n",
      "Mean Specificity: 0.8185\n",
      "\n",
      "\n",
      "Performance Metrics for task_23:\n",
      "Mean Accuracy: 0.8392\n",
      "Mean Precision: 0.8190\n",
      "Mean Recall: 0.8963\n",
      "Mean F1 Score: 0.8518\n",
      "Mean Sensitivity: 0.8963\n",
      "Mean Specificity: 0.7772\n",
      "\n",
      "\n",
      "Performance Metrics for task_24:\n",
      "Mean Accuracy: 0.5919\n",
      "Mean Precision: 0.6825\n",
      "Mean Recall: 0.5108\n",
      "Mean F1 Score: 0.5400\n",
      "Mean Sensitivity: 0.5108\n",
      "Mean Specificity: 0.6998\n",
      "\n",
      "\n",
      "Performance Metrics for task_25:\n",
      "Mean Accuracy: 0.6780\n",
      "Mean Precision: 0.6451\n",
      "Mean Recall: 0.8112\n",
      "Mean F1 Score: 0.7178\n",
      "Mean Sensitivity: 0.8112\n",
      "Mean Specificity: 0.5173\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Performance evaluation with K-Fold cross-validation\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Number of folds\n",
    "n_splits = 5  # You can adjust the number of folds as needed\n",
    "\n",
    "# Dictionary to store performance metrics for each task\n",
    "performance_metrics = {}\n",
    "\n",
    "# Using the best parameters computed during grid-search\n",
    "best_params = {\n",
    "    'criterion': 'gini', \n",
    "    'max_depth': 2,\n",
    "    'max_leaf_nodes': 2, \n",
    "    'min_samples_leaf': 2, \n",
    "    'min_samples_split': 2\n",
    "}\n",
    "\n",
    "# Iterate through tasks\n",
    "for task, task_df in task_dfs.items():\n",
    "    X_task = task_df\n",
    "    y_task = task_labels[task]\n",
    "\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    sensitivities = []\n",
    "    specificities = []\n",
    "\n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=None)\n",
    "\n",
    "    for train_index, test_index in kf.split(X_task):\n",
    "        X_train, X_test = X_task.iloc[train_index], X_task.iloc[test_index]\n",
    "        y_train, y_test = y_task.iloc[train_index], y_task.iloc[test_index]\n",
    "\n",
    "        # Create classifier with best parameters\n",
    "        clf = DecisionTreeClassifier(**best_params)\n",
    "\n",
    "        # Train the model\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, pos_label='P')\n",
    "        recall = recall_score(y_test, y_pred, pos_label='P')\n",
    "        f1 = f1_score(y_test, y_pred, pos_label='P')\n",
    "\n",
    "        # Calculate confusion matrix components\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=['H', 'P'])\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        # Calculate sensitivity and specificity\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "\n",
    "        # Append metrics\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        sensitivities.append(sensitivity)\n",
    "        specificities.append(specificity)\n",
    "\n",
    "    # Calculate average metrics\n",
    "    performance_metrics[task] = {\n",
    "        'mean_accuracy': np.mean(accuracies),\n",
    "        'mean_precision': np.mean(precisions),\n",
    "        'mean_recall': np.mean(recalls),\n",
    "        'mean_f1_score': np.mean(f1_scores),\n",
    "        'mean_sensitivity': np.mean(sensitivities),\n",
    "        'mean_specificity': np.mean(specificities)\n",
    "    }\n",
    "\n",
    "    print(f\"Performance Metrics for {task}:\")\n",
    "    print(f\"Mean Accuracy: {performance_metrics[task]['mean_accuracy']:.4f}\")\n",
    "    print(f\"Mean Precision: {performance_metrics[task]['mean_precision']:.4f}\")\n",
    "    print(f\"Mean Recall: {performance_metrics[task]['mean_recall']:.4f}\")\n",
    "    print(f\"Mean F1 Score: {performance_metrics[task]['mean_f1_score']:.4f}\")\n",
    "    print(f\"Mean Sensitivity: {performance_metrics[task]['mean_sensitivity']:.4f}\")\n",
    "    print(f\"Mean Specificity: {performance_metrics[task]['mean_specificity']:.4f}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6bf112-bc77-443d-b79c-56c6871b2e54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
